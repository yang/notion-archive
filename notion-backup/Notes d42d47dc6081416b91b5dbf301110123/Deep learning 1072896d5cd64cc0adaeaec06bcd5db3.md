# Deep learning

- Architectures
    - Transformers = positional encoding + attention + self-attention
        - Used by BERT, GPT, T5, etc.
        - Originally for translation
        - TODO What is self-attention?
        - Resources
            - Paper: [[1706.03762] Attention Is All You Need](https://arxiv.org/abs/1706.03762)
            - Google talk intuition: [Transformers, explained: Understand the model behind GPT, BERT, and T5 - YouTube](https://www.youtube.com/watch?v=SZorAJ4I-sA)
    - Attention
        - Originally for translation
        - Resources
            - Andrew Ng intuition: [https://www.youtube.com/watch?v=ZU12u6-ewP0](https://www.youtube.com/watch?v=ZU12u6-ewP0)
    - Encoder-decoder
    - Autoencoder
    - GAN
    - RNNs have to consume all input before producing output, so are limited by ability to memorize things