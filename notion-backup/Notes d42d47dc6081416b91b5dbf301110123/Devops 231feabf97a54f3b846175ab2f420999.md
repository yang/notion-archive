# Devops

# To learn

There are so many features in the Linux kernel it sometimes blows my mind. eventfd, signalfd, timerfd, memfd, pidfd. The whole fricking tc/qdisc featureset (OMG). netlink. io_uring. criu. SO_REUSEPORT. Teaming. Namespaces. veths. vsocks. Dpdk/netmap/af_packet. XDP ! Seccomp. ([source](https://news.ycombinator.com/item?id=27287930))

# Tools

- bcc-tools
    - execsnoop: find periodic running processes
    - opensnoop: misconfigs
    - tcplife: unexpected TCP sessions
    - ext4slower: any IO slower than 10ms
    - biosnoop: unusual patterns
- bpftrace
- flamescope

# Cloud tools

- [https://steampipe.io/](https://steampipe.io/): SQL interface to APIs

# Linux security

- Capabilities are superuser (privileged) powers broken up

# Containers landscape

- Modern tools
    - containerd: manage/supervise containers, uses runc; spun out from Docker; used in k8s
    - runc: the actual runner of a single container
        - Kind of like systemd-nspawn
        - Supports rootless
    - CRI-O: lighterweight alternative to containerd; used in k8s
- Secondary tools
    - runsc: gvisor
    - kaniko (google)
    - buildkit
    - Container Tools project: alternative to Docker
        - buildah: building
        - podman: running
        - skopeo: distributing
- Standards
    - OCI: standard image format
    - CRI: standard runtime interface
- Obsolete tools
    - systemd-nspawn
    - Docker
    - lxc, lxd
    - rkt
    - coreos container linux
- Resources
    - Great overview [https://www.capitalone.com/tech/cloud/container-runtime/](https://www.capitalone.com/tech/cloud/container-runtime/)

# Linux namespaces

- (User) namespaces can be nested up to 32 levels ([source](https://man7.org/linux/man-pages/man7/user_namespaces.7.html))
- Resources
    - Fantastic talk on namespaces ([video](https://www.youtube.com/watch?v=83NOk8pmHi8&list=WL&index=3&t=1086s))
    - Great live coding a container ([video](https://www.youtube.com/watch?v=8fi7uSYlOdc))
- slirp4netns: userland network stack for unprivileged network namespaces

## User namespace

- UID/GID map
- All entities must exist outside

## Tools

- CLI
    - unshare
    - nsenter
- System calls
    - unshare
    - clone can specify namespaces
    - setns

# Sandboxing

- seccomp vs capabilities: capabilities are coarse grains, syscalls check them themselves, whereas SEC comp filters system calls from starting [https://security.stackexchange.com/questions/210400/difference-between-linux-capabities-and-seccomp](https://security.stackexchange.com/questions/210400/difference-between-linux-capabities-and-seccomp)
- In general, container escapes are considered CVEs, but it’s fairly risky due to the kernel’s large surface area.

## Tools

- containers
    - unshare
- systemd
    - systemd-nspawn
        - Apply set comp filters without access to source code
        - However, still allows exec, which is common for starting shells. Needed since the filters must be applied before starting the new program
        - Also requires root?
    - machinectl
- [sandbox](https://github.com/cloudflare/sandbox) from Cloudflare ([blog](https://blog.cloudflare.com/sandboxing-in-linux-with-zero-lines-of-code/))
    - Apply seccomp filters without access to source code
    - Does not require exact since it runs post initialization. Requires either dynamic linking or modifying the elf binary
- [firejail](https://github.com/netblue30/firejail)
    - Uses namespaces and seccomp
    - Large, focuses on desktop apps and end-users
    - It secures the programs that you target, but being an suid binary means others on your host can find exploits and Gain root privileges more easily
    - Vs bubblewrap: [https://github.com/netblue30/firejail/discussions/4522](https://github.com/netblue30/firejail/discussions/4522)
    - Does not play well with containers
- [bubblewrap](https://github.com/containers/bubblewrap) - nice compact tool
    - Uses namespaces and seccomp
    - Does not require setuid, unlike firejail
    - Use user space network stack to provide partial network access: [discussion](https://github.com/containers/bubblewrap/issues/504)
- Minijail [https://android.googlesource.com/platform/external/minijail/](https://android.googlesource.com/platform/external/minijail/)
    - Apply seccomp filters without access to source code
    - Also apply other things as well including namespaces and capabilities
- Mbox [https://github.com/tsgates/mbox](https://github.com/tsgates/mbox)
    - Does not add any privacy by default, only prevents mutations
    - Prints network activity
    - Allows selecting changes to commit back to the host file system
    - Both seccomp and ptrace implantations
    - Paper and slides have very interesting performance measurements of the overhead of using SECcomp and pTrace
- Others
    - crabjail [https://codeberg.org/crabjail/crabjail](https://codeberg.org/crabjail/crabjail)
    - nsjail [https://nsjail.dev/](https://nsjail.dev/)
    - 
- Analysis
    - [https://github.com/shamedgh/confine](https://github.com/shamedgh/confine)
    - [https://github.com/david942j/seccomp-tools](https://github.com/david942j/seccomp-tools)
    - [https://github.com/containers/oci-seccomp-bpf-hook](https://github.com/containers/oci-seccomp-bpf-hook)
    - [https://github.com/kinvolk/inspektor-gadget](https://github.com/kinvolk/inspektor-gadget)
- Resources
    - [https://chromium.googlesource.com/chromium/src.git/+/HEAD/docs/linux/sandboxing.md](https://chromium.googlesource.com/chromium/src.git/+/HEAD/docs/linux/sandboxing.md)
    - [Discussion](https://news.ycombinator.com/item?id=16975706)
    - [https://news.ycombinator.com/item?id=30820646](https://news.ycombinator.com/item?id=30820646)
    - [https://hkubota.wordpress.com/2020/12/31/comparing-sandboxing-tools/](https://hkubota.wordpress.com/2020/12/31/comparing-sandboxing-tools/)

## Recommendations

- Let's say you are building a service that needs to evaluate untrusted user code per request. You already have a kubernetes cluster.
- Approaches
    - Start a new pod per request, but k8a is not really designed to be secure, need to check with the namespace sharing defaults are, and you still have the full kernel API available. It's also not very low latency.
    - Run outside of kubernetes in a firejail or bubblewrap, but this does not get to reuse your kubernetes cluster.
    - Use something based on firecracker. Give up on running EKS, need bare metal for firecracker.
    - Use AWS lambda. Lambda might have larger cold start times, and cost per request. Not sure how to enforce different execution environment per request.
    - If you are using v8, then you can use V8 isolates with something like isolated-VM, or you can use vm2. Extremely low latency.

# VMs, containers, k8s

These require bare metal to run

## firecracker

- firecracker-containers

## kata-containers

- 

## gvisor

- does not require bare metal

## Lambda

- Execution environments are never reused across different function versions or customers, but a single environment can be reused between invocations of the same function version. This means data and state can persist between invocations.

# Containers

- CRIU to take snapshots. Slow, multi second. In memory copy on write forking is research.

# Kafka

- concepts
    - **broker aka node**: a cluster is built by one or more servers. The servers forming the storage layer are called brokers
    - **event**: have sequential IDs? key, value, timestamp, and optional metadata headers.
    - **producers**: client applications that publish (write) events to Kafka
    - **consumer**: client application subscribed to (read and process) events from Kafka
    - **topic**: group of events
    - **partition**: topics are partitioned, meaning a topic is spread over a number of "buckets" located on different Kafka brokers
    - **replication**: to make your data fault-tolerant and highly-available, every topic can be replicated, so that there are always multiple brokers that have a copy
- default retention is 7 days or 1 gb, whichever is lower ([source](https://www.cloudkarafka.com/blog/what-is-kafka-retention-period.html))

## Tips

**Inspect topics and partitions**

```
kafkacat -b localhost:9092 -L

```

Example output:

```
Metadata for all topics (from broker -1: localhost:9092/bootstrap):
 3 brokers:
  broker 2 at b-2.plasmic-posthog-kafka.timval.c7.kafka.us-west-2.amazonaws.com:9092
  broker 3 at b-3.plasmic-posthog-kafka.timval.c7.kafka.us-west-2.amazonaws.com:9092 (controller)
  broker 1 at b-1.plasmic-posthog-kafka.timval.c7.kafka.us-west-2.amazonaws.com:9092
 10 topics:
  topic "events_dead_letter_queue" with 5 partitions:
    partition 0, leader 1, replicas: 1,3,2, isrs: 3,2,1
    partition 1, leader 3, replicas: 3,2,1, isrs: 3,2,1
    partition 2, leader 2, replicas: 2,1,3, isrs: 3,2,1
    partition 3, leader 1, replicas: 1,2,3, isrs: 3,2,1
    partition 4, leader 3, replicas: 3,1,2, isrs: 3,2,1
  topic "readings" with 5 partitions:
    partition 0, leader 3, replicas: 3,2,1, isrs: 3,2,1
    partition 1, leader 2, replicas: 2,1,3, isrs: 3,2,1
    partition 2, leader 1, replicas: 1,3,2, isrs: 3,2,1
    partition 3, leader 3, replicas: 3,1,2, isrs: 3,2,1
    partition 4, leader 2, replicas: 2,3,1, isrs: 3,2,1
  topic "clickhouse_person_unique_id" with 5 partitions:
    partition 0, leader 1, replicas: 1,3,2, isrs: 3,2,1
    partition 1, leader 3, replicas: 3,2,1, isrs: 3,2,1
    partition 2, leader 2, replicas: 2,1,3, isrs: 3,2,1
    partition 3, leader 1, replicas: 1,2,3, isrs: 3,2,1
    partition 4, leader 3, replicas: 3,1,2, isrs: 3,2,1
  topic "__consumer_offsets" with 50 partitions:
    partition 0, leader 1, replicas: 1,3,2, isrs: 3,2,1
    partition 1, leader 3, replicas: 3,2,1, isrs: 3,2,1
    partition 2, leader 2, replicas: 2,1,3, isrs: 3,2,1
    partition 3, leader 1, replicas: 1,2,3, isrs: 3,2,1
    partition 4, leader 3, replicas: 3,1,2, isrs: 3,2,1
    partition 5, leader 2, replicas: 2,3,1, isrs: 3,2,1
    partition 6, leader 1, replicas: 1,3,2, isrs: 3,2,1
    partition 7, leader 3, replicas: 3,2,1, isrs: 3,2,1
    partition 8, leader 2, replicas: 2,1,3, isrs: 3,2,1
...

```

**L****ist consumer groups**

```
./bin/kafka-consumer-groups.sh --list --bootstrap-server localhost:9092

```

Example output;

```
amazon.msk.canary.group.broker-1
clickhouse-ingestion
felipe_group
readings_consumer_group1
felipe_group3
felipe_group2
group2
amazon.msk.canary.group.broker-3
amazon.msk.canary.group.broker-2
group1

```

Inspect consumer group

```
./bin/kafka-consumer-groups.sh --describe --group mygroup --bootstrap-server localhost:9092

```

Example output:

```
root@my-shell:~/kafka_2.12-2.6.2# ./bin/kafka-consumer-groups.sh --describe --group group1 --bootstrap-server b-3.plasmic-posthog-kafka.timval.c7.kafka.us-west-2.amazonaws.com:9092,b-2.plasmic-posthog-kafka.timval.c7.kafka.us-west-2.amazonaws.com:9092,b-1.plasmic-posthog-kafka.timval.c7.kafka.us-west-2.amazonaws.com:9092

GROUP           TOPIC                         PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  LAG             CONSUMER-ID                                                                                                                                                        HOST            CLIENT-ID
group1          clickhouse_person_unique_id   2          -               17              -               ClickHouse-chi-posthog-posthog-0-0-0.chi-posthog-posthog-0-0.posthog.svc.cluster.local-posthog-kafka_person_distinct_id-03732755-51bc-4cec-91db-a37adc2079f5       /192.168.68.145 ClickHouse-chi-posthog-posthog-0-0-0.chi-posthog-posthog-0-0.posthog.svc.cluster.local-posthog-kafka_person_distinct_id
group1          clickhouse_person_unique_id   0          -               15              -               ClickHouse-chi-posthog-posthog-0-0-0.chi-posthog-posthog-0-0.posthog.svc.cluster.local-posthog-kafka_person_distinct_id-03732755-51bc-4cec-91db-a37adc2079f5       /192.168.68.145 ClickHouse-chi-posthog-posthog-0-0-0.chi-posthog-posthog-0-0.posthog.svc.cluster.local-posthog-kafka_person_distinct_id
group1          clickhouse_person_unique_id   4          -               13              -               ClickHouse-chi-posthog-posthog-0-0-0.chi-posthog-posthog-0-0.posthog.svc.cluster.local-posthog-kafka_person_distinct_id-03732755-51bc-4cec-91db-a37adc2079f5       /192.168.68.145 ClickHouse-chi-posthog-posthog-0-0-0.chi-posthog-posthog-0-0.posthog.svc.cluster.local-posthog-kafka_person_distinct_id
group1          clickhouse_person_unique_id   3          -               21              -               ClickHouse-chi-posthog-posthog-0-0-0.chi-posthog-posthog-0-0.posthog.svc.cluster.local-posthog-kafka_person_distinct_id-03732755-51bc-4cec-91db-a37adc2079f5       /192.168.68.145 ClickHouse-chi-posthog-posthog-0-0-0.chi-posthog-posthog-0-0.posthog.svc.cluster.local-posthog-kafka_person_distinct_id
group1          clickhouse_person_unique_id   1          -               20              -               ClickHouse-chi-posthog-posthog-0-0-0.chi-posthog-posthog-0-0.posthog.svc.cluster.local-posthog-kafka_person_distinct_id-03732755-51bc-4cec-91db-a37adc2079f5       /192.168.68.145 ClickHouse-chi-posthog-posthog-0-0-0.chi-posthog-posthog-0-0.posthog.svc.cluster.local-posthog-kafka_person_distinct_id
group1          plugin_log_entries            4          946             962             16              ClickHouse-chi-posthog-posthog-0-0-0.chi-posthog-posthog-0-0.posthog.svc.cluster.local-posthog-kafka_plugin_log_entries-e97dff5a-34da-4b3d-8890-b395f0f935fd       /192.168.68.145 ClickHouse-chi-posthog-posthog-0-0-0.chi-posthog-posthog-0-0.posthog.svc.cluster.local-posthog-kafka_plugin_log_entries
group1          plugin_log_entries            1          926             946             20              ClickHouse-chi-posthog-posthog-0-0-0.chi-posthog-posthog-0-0.posthog.svc.cluster.local-posthog-kafka_plugin_log_entries-e97dff5a-34da-4b3d-8890-b395f0f935fd       /192.168.68.145 ClickHouse-chi-posthog-posthog-0-0-0.chi-posthog-posthog-0-0.posthog.svc.cluster.local-posthog-kafka_plugin_log_entries
group1          plugin_log_entries            0          818             832             14              ClickHouse-chi-posthog-posthog-0-0-0.chi-posthog-posthog-0-0.posthog.svc.cluster.local-posthog-kafka_plugin_log_entries-e97dff5a-34da-4b3d-8890-b395f0f935fd       /192.168.68.145 ClickHouse-chi-posthog-posthog-0-0-0.chi-posthog-posthog-0-0.posthog.svc.cluster.local-posthog-kafka_plugin_log_entries
group1          plugin_log_entries            2          890             899             9               ClickHouse-chi-posthog-posthog-0-0-0.chi-posthog-posthog-0-0.posthog.svc.cluster.local-posthog-kafka_plugin_log_entries-e97dff5a-34da-4b3d-8890-b395f0f935fd       /192.168.68.145 ClickHouse-chi-posthog-posthog-0-0-0.chi-posthog-posthog-0-0.posthog.svc.cluster.local-posthog-kafka_plugin_log_entries
group1          plugin_log_entries            3          845             858             13              ClickHouse-chi-posthog-posthog-0-0-0.chi-posthog-posthog-0-0.posthog.svc.cluster.local-posthog-kafka_plugin_log_entries-e97dff5a-34da-4b3d-8890-b395f0f935fd       /192.168.68.145 ClickHouse-chi-posthog-posthog-0-0-0.chi-posthog-posthog-0-0.posthog.svc.cluster.local-posthog-kafka_plugin_log_entries
group1          clickhouse_person             3          19132           19536           404             ClickHouse-chi-posthog-posthog-0-0-0.chi-posthog-posthog-0-0.posthog.svc.cluster.local-posthog-kafka_person-dcfda8bc-a262-49ef-9da7-bc49176d919f                   /192.168.68.145 ClickHouse-chi-posthog-posthog-0-0-0.chi-posthog-posthog-0-0.posthog.svc.cluster.local-posthog-kafka_person
group1          clickhouse_person             2          19271           19669           398             ClickHouse-chi-posthog-posthog-0-0-0.chi-posthog-posthog-0-0.posthog.svc.cluster.local-posthog-kafka_person-dcfda8bc-a262-49ef-9da7-bc49176d919f                   /192.168.68.145 ClickHouse-chi-posthog-posthog-0-0-0.chi-posthog-posthog-0-0.posthog.svc.cluster.local-posthog-kafka_person
group1          clickhouse_person             0          19300           19698           398             ClickHouse-chi-posthog-posthog-0-0-0.chi-posthog-posthog-0-0.posthog.svc.cluster.local-posthog-kafka_person-dcfda8bc-a262-49ef-9da7-bc49176d919f                   /192.168.68.145 ClickHouse-chi-posthog-posthog-0-0-0.chi-posthog-posthog-0-0.posthog.svc.cluster.local-posthog-kafka_person
group1          clickhouse_person             1          19199           19616           417             ClickHouse-chi-posthog-posthog-0-0-0.chi-posthog-posthog-0-0.posthog.svc.cluster.local-posthog-kafka_person-dcfda8bc-a262-49ef-9da7-bc49176d919f                   /192.168.68.145 ClickHouse-chi-posthog-posthog-0-0-0.chi-posthog-posthog-0-0.posthog.svc.cluster.local-posthog-kafka_person
group1          clickhouse_person             4          19163           19569           406             ClickHouse-chi-posthog-posthog-0-0-0.chi-posthog-posthog-0-0.posthog.svc.cluster.local-posthog-kafka_person-dcfda8bc-a262-49ef-9da7-bc49176d919f                   /192.168.68.145 ClickHouse-chi-posthog-posthog-0-0-0.chi-posthog-posthog-0-0.posthog.svc.cluster.local-posthog-kafka_person
group1          events_dead_letter_queue      1          38              170             132             ClickHouse-chi-posthog-posthog-0-0-0.chi-posthog-posthog-0-0.posthog.svc.cluster.local-posthog-kafka_events_dead_letter_queue-8083d49f-c495-485a-b55e-416bf4325ab3 /192.168.68.145 ClickHouse-chi-posthog-posthog-0-0-0.chi-posthog-posthog-0-0.posthog.svc.cluster.local-posthog-kafka_events_dead_letter_queue
group1          events_dead_letter_queue      4          26              157             131             ClickHouse-chi-posthog-posthog-0-0-0.chi-posthog-posthog-0-0.posthog.svc.cluster.local-posthog-kafka_events_dead_letter_queue-8083d49f-c495-485a-b55e-416bf4325ab3 /192.168.68.145 ClickHouse-chi-posthog-posthog-0-0-0.chi-posthog-posthog-0-0.posthog.svc.cluster.local-posthog-kafka_events_dead_letter_queue
group1          events_dead_letter_queue      2          32              140             108             ClickHouse-chi-posthog-posthog-0-0-0.chi-posthog-posthog-0-0.posthog.svc.cluster.local-posthog-kafka_events_dead_letter_queue-8083d49f-c495-485a-b55e-416bf4325ab3 /192.168.68.145 ClickHouse-chi-posthog-posthog-0-0-0.chi-posthog-posthog-0-0.posthog.svc.cluster.local-posthog-kafka_events_dead_letter_queue
group1          events_dead_letter_queue      0          35              186             151             ClickHouse-chi-posthog-posthog-0-0-0.chi-posthog-posthog-0-0.posthog.svc.cluster.local-posthog-kafka_events_dead_letter_queue-8083d49f-c495-485a-b55e-416bf4325ab3 /192.168.68.145 ClickHouse-chi-posthog-posthog-0-0-0.chi-posthog-posthog-0-0.posthog.svc.cluster.local-posthog-kafka_events_dead_letter_queue
group1          events_dead_letter_queue      3          36              181             145             ClickHouse-chi-posthog-posthog-0-0-0.chi-posthog-posthog-0-0.posthog.svc.cluster.local-posthog-kafka_events_dead_letter_queue-8083d49f-c495-485a-b55e-416bf4325ab3 /192.168.68.145 ClickHouse-chi-posthog-posthog-0-0-0.chi-posthog-posthog-0-0.posthog.svc.cluster.local-posthog-kafka_events_dead_letter_queue
group1          clickhouse_events_proto       1          2971117         2978762         7645            ClickHouse-chi-posthog-posthog-0-0-0.chi-posthog-posthog-0-0.posthog.svc.cluster.local-posthog-kafka_events-76b6d2a8-08d3-485b-9c98-7edd2ee929d6                   /192.168.68.145 ClickHouse-chi-posthog-posthog-0-0-0.chi-posthog-posthog-0-0.posthog.svc.cluster.local-posthog-kafka_events
group1          clickhouse_events_proto       2          4471988         4480489         8501            ClickHouse-chi-posthog-posthog-0-0-0.chi-posthog-posthog-0-0.posthog.svc.cluster.local-posthog-kafka_events-76b6d2a8-08d3-485b-9c98-7edd2ee929d6                   /192.168.68.145 ClickHouse-chi-posthog-posthog-0-0-0.chi-posthog-posthog-0-0.posthog.svc.cluster.local-posthog-kafka_events
group1          clickhouse_events_proto       4          4447418         4464073         16655           ClickHouse-chi-posthog-posthog-0-0-0.chi-posthog-posthog-0-0.posthog.svc.cluster.local-posthog-kafka_events-76b6d2a8-08d3-485b-9c98-7edd2ee929d6                   /192.168.68.145 ClickHouse-chi-posthog-posthog-0-0-0.chi-posthog-posthog-0-0.posthog.svc.cluster.local-posthog-kafka_events
group1          clickhouse_events_proto       0          5404052         5413449         9397            ClickHouse-chi-posthog-posthog-0-0-0.chi-posthog-posthog-0-0.posthog.svc.cluster.local-posthog-kafka_events-76b6d2a8-08d3-485b-9c98-7edd2ee929d6                   /192.168.68.145 ClickHouse-chi-posthog-posthog-0-0-0.chi-posthog-posthog-0-0.posthog.svc.cluster.local-posthog-kafka_events
group1          clickhouse_events_proto       3          4733941         4753470         19529           ClickHouse-chi-posthog-posthog-0-0-0.chi-posthog-posthog-0-0.posthog.svc.cluster.local-posthog-kafka_events-76b6d2a8-08d3-485b-9c98-7edd2ee929d6                   /192.168.68.145 ClickHouse-chi-posthog-posthog-0-0-0.chi-posthog-posthog-0-0.posthog.svc.cluster.local-posthog-kafka_events
group1          clickhouse_person_distinct_id 3          19220           19621           401             ClickHouse-chi-posthog-posthog-0-0-0.chi-posthog-posthog-0-0.posthog.svc.cluster.local-posthog-kafka_person_distinct_id2-1d62e42b-749e-4aff-a0f8-6d0755f0a5b2      /192.168.68.145 ClickHouse-chi-posthog-posthog-0-0-0.chi-posthog-posthog-0-0.posthog.svc.cluster.local-posthog-kafka_person_distinct_id2
group1          clickhouse_person_distinct_id 4          19133           19522           389             ClickHouse-chi-posthog-posthog-0-0-0.chi-posthog-posthog-0-0.posthog.svc.cluster.local-posthog-kafka_person_distinct_id2-1d62e42b-749e-4aff-a0f8-6d0755f0a5b2      /192.168.68.145 ClickHouse-chi-posthog-posthog-0-0-0.chi-posthog-posthog-0-0.posthog.svc.cluster.local-posthog-kafka_person_distinct_id2
group1          clickhouse_person_distinct_id 1          19185           19582           397             ClickHouse-chi-posthog-posthog-0-0-0.chi-posthog-posthog-0-0.posthog.svc.cluster.local-posthog-kafka_person_distinct_id2-1d62e42b-749e-4aff-a0f8-6d0755f0a5b2      /192.168.68.145 ClickHouse-chi-posthog-posthog-0-0-0.chi-posthog-posthog-0-0.posthog.svc.cluster.local-posthog-kafka_person_distinct_id2
group1          clickhouse_person_distinct_id 0          19049           19457           408             ClickHouse-chi-posthog-posthog-0-0-0.chi-posthog-posthog-0-0.posthog.svc.cluster.local-posthog-kafka_person_distinct_id2-1d62e42b-749e-4aff-a0f8-6d0755f0a5b2      /192.168.68.145 ClickHouse-chi-posthog-posthog-0-0-0.chi-posthog-posthog-0-0.posthog.svc.cluster.local-posthog-kafka_person_distinct_id2
group1          clickhouse_person_distinct_id 2          19078           19490           412             ClickHouse-chi-posthog-posthog-0-0-0.chi-posthog-posthog-0-0.posthog.svc.cluster.local-posthog-kafka_person_distinct_id2-1d62e42b-749e-4aff-a0f8-6d0755f0a5b2      /192.168.68.145 ClickHouse-chi-posthog-posthog-0-0-0.chi-posthog-posthog-0-0.posthog.svc.cluster.local-posthog-kafka_person_distinct_id2

```

# Observability

- Trickier to instrument Node.js console.log with span context, since there are no thread-locals. Probably need tooling based on async_hooks (which is the new way of doing zones/domains/continuation-local-storage).
- Jaeger supports span-associated logging, but only for specific platforms (not Node.js) and only using the jaeger client libraries (not opentelemetry AFAIK).

# ElasticSearch

- Types are globally shared across all document types in an index. You can’t have two document types that each have a different type (string vs date) for their `created` field, for instance. ([source](https://www.elastic.co/guide/en/elasticsearch/reference/current/removal-of-types.html))

# GitHub Actions

## Auto PR on branch push

- [https://stackoverflow.com/questions/68057744/create-pull-request-with-github-action](https://stackoverflow.com/questions/68057744/create-pull-request-with-github-action)
- [https://github.com/vsoch/pull-request-action](https://github.com/vsoch/pull-request-action)

# Stack

- CI
    - GitHub Actions: has limits on e.g. caching, popular, can have self-hosted runners
    - GitLab CI: open source / self-hostable, still reasonably popular
    - Jenkins X: pure-k8s solution, limited to certain git hosting providers (e.g. no Gerrit), not as popular as Jenkins
- CD TODO
    - Not sure really need anything more than k8s yaml’s at this point, rollouts are pretty simple
    - Main complexity is in DB schema migrations
- Logging TODO
    - Fluentd: collects log files
    - Elastic: requires application to log using Elastic library? (source: Nana fluentd video)
- Monitoring TODO
    - Prometheus
- Error reporting
    - Sentry
- DB
    - PostgreSQL: nice DX
    - MySQL: nicer operationally; has more mature replication, gh-ost for online schema changes, Vitess for sharding, etc.
    - CockroachDB: more popular than TiDB
    - TiDB: doesn’t have full serializable transactions
    - FaunaDB: maybe? Non-SQL
    - PlanetScale: interesting branching system, would like to learn more

# CI

- Open
    - Jenkins (X) - terrible docs, probably terrible platform (if Jenkins is any indication)
    - Concourse
    - Drone - like a more platform independent GitHub Actions
    - GitLab CI
- Proprietary
    - GitHub Actions
- Building blocks
    - Tekton pipeline
    - Argo Workflows

# Docker tools

# AWS

## S3

Tips:

- Make a single object public, assuming the bucket has enabled individual object ACLs: `aws s3 sync ./local-folder-name s3://remote-bucket-name --acl=public-read`
- Make a whole bucket public ([source](https://bobbyhadz.com/blog/aws-s3-allow-public-read-access)):

```jsx
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": "*",
      "Action": [
        "s3:GetObject"
      ],
      "Resource": "arn:aws:s3:::YOUR_BUCKET_NAME/*"
    }
  ]
}
```

## CloudFront

- Standard logs delivered up some number of times per hour ([source](https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/AccessLogs.html))
- Also has real-time logs

## EC2

- m5
    - m5a: AMD (10% cheaper)
- m6: ARM
    - m6g: Graviton

## Services

- EKS: managed Kubernetes control plane; ensures availability, backup, etc. (“RDS for kubeadm”)
- ECS: AWS-proprietary container cluster orchestration; no reason to use over EKS
- Fargate: serverless compute engine; works with EKS & ECS
- ECR: container registry
- ASG: simply manages EC2/EBS (and other resources), put behind load balancer
- Lambda: FaaS; can run containers
- Elastic Beanstalk: PaaS; can run containers
- CloudFormation: provision any AWS resources as a code template

## Pricing (2021-04-10)

- Compute
    - Fargate: $0.04048/vCPU/hr + $0.004445/GB/hr; pricing is per-sec
        - For 2vCPU+8GB = $0.11652/hr = $84/mo
    - Lambda: $0.0000166667/GB/s (at 1,792 MB we get 1 full vCPU)
        - For 8GB: $345.60/mo (non-stop)
        - Best for low-utilization
    - EC2: m5a.large (2vCPU+8GB) = $0.086/hr = $62/mo
        - Reserved: $0.051/hr = $37/mo
    - GitHub Actions: $0.008/min = $0.48/hr (2-core CPUs, 7GB RAM)
    - For:
        - 20,480 requests = 32 e2e tests * 32 CRs per day * 20 work days
        - 300,000 ms = 1,000 ms * 60 s * 5 min
        - 4GB RAM
        - Lambda: $409.60/mo
- EKS: $0.10/cluster/hr ($72/mo)
- Beanstalk: free, just pay for underlying resources

## Fargate

- ~1m cold start ([source](https://www.reddit.com/r/aws/comments/8s9n9m/im_confused_about_fargate_does_it_run/e0yr90j/?utm_source=reddit&utm_medium=web2x&context=3))
- Supports EFS now ([source](https://aws.amazon.com/about-aws/whats-new/2020/04/amazon-ecs-aws-fargate-support-amazon-efs-filesystems-generally-available/))

## VPC

- [Tutorial](https://dev.to/fon/a-simple-vpc-peering-tutorial-48mo).
- Wasn’t able to [reference SGs](https://docs.aws.amazon.com/vpc/latest/peering/vpc-peering-security-groups.html), had to paste CIDR blocks into SG rules.
- Peering lets you transfer between same-AZ VPCs for free ([source](https://aws.amazon.com/blogs/architecture/overview-of-data-transfer-costs-for-common-architectures/))

## EKS

- Easiest way to use is eksctl ([docs](https://docs.aws.amazon.com/eks/latest/userguide/getting-started-eksctl.html))
- Terraform might be the nicest way to manage EKS?
    - eksctl does not have an `apply` even though it can read from config files ([issue](https://github.com/weaveworks/eksctl/issues/2774))
    - I think the main way to update cluster/nodegroup configs is via the AWS management console
    - eksctl delete nodegroup has an annoying bug ([issue](https://github.com/weaveworks/eksctl/issues/1849)—workaround is to first delete the Network Interface)
- Gotchas:
    - `eksctl create cluster` `--``fargate` actually sets coredns annotation to run on fargate rather than ec2! ([docs](https://docs.aws.amazon.com/eks/latest/userguide/fargate-getting-started.html#fargate-gs-coredns))
- Fargate slow startup time causes timeouts when doing anything, such as `kubectl run` or `helm install`
- ssh into nodes as ec2-user@
- Node groups: sounds like an ASG of EC2 instances (can autoscale) ([source](https://www.youtube.com/watch?v=p6xDCz00TxU&list=WL&index=51))
    - Only recently added ability to specify taints
- Fargate profiles
    - Are immutable
    - Cannot specify taints
- To grant IAM users/roles access to the cluster, an IAM policy like this must be created and attached ([docs](https://docs.aws.amazon.com/eks/latest/userguide/security_iam_id-based-policy-examples.html#security_iam_id-based-policy-examples-console)):
    
    ```jsx
    {
        "Version": "2012-10-17",
        "Statement": [
            {
                "Sid": "VisualEditor0",
                "Effect": "Allow",
                "Action": "eks:*",
                "Resource": "*"
            }
        ]
    }
    ```
    
    - The cluster creator must add them to the aws-auth configmap ([source](https://aws.amazon.com/premiumsupport/knowledge-center/eks-api-server-unauthorized-error/)):
        
        ```jsx
        kubectl edit configmap aws-auth -n kube-system
        
        mapUsers: |
          - userarn: arn:aws:iam::XXXXXXXXXXXX:user/testuser
            username: testuser
            groups:
              - system:masters
        # OR
        mapRoles: |
          - rolearn: arn:aws:iam::XXXXXXXXXXXX:role/testrole
            username: testrole
            groups:
              - system:masters
        ```
        
    - Then add the user also needs to initialize kubectl config from their client host:
        
        ```jsx
        aws eks --region region update-kubeconfig --name cluster_name
        ```
        
    
    # OR
    
- To deploy an HTTPS load balancer ([source](https://aws.amazon.com/premiumsupport/knowledge-center/terminate-https-traffic-eks-acm/)):
    - Get certificate via ACM, and copy its ARN
    - Apply this service manifest
        
        ```jsx
        apiVersion: v1
        kind: Service
        metadata:
          name: echo-service
          annotations:
            # Note that the backend talks over HTTP.
            service.beta.kubernetes.io/aws-load-balancer-backend-protocol: http
            # TODO: Fill in with the ARN of your certificate.
            service.beta.kubernetes.io/aws-load-balancer-ssl-cert: arn:aws:acm:{region}:{user id}:certificate/{id}
            # Only run SSL on the port named "https" below.
            service.beta.kubernetes.io/aws-load-balancer-ssl-ports: "https"
        spec:
          selector:
            app: echo-pod
          ports:
          - name: http
            port: 80
            targetPort: 8080
          - name: https
            port: 443
            targetPort: 8080
          type: LoadBalancer
        ```
        
    - After deployment, create a CNAME pointing to the ELB
- Max pod limits per instance type: [https://github.com/awslabs/amazon-eks-ami/blob/master/files/eni-max-pods.txt](https://github.com/awslabs/amazon-eks-ami/blob/master/files/eni-max-pods.txt)
- Gripes:
    - Costs money
    - Setup is slow
    - Can’t add IAM groups, only individual IAM users, to aws-auth

## Comparisons

- ECS & EKS are both control planes for container clusters
    - Worker plane can be:
        - EC2 (unmanaged)
        - EKS node groups (semi-managed)
        - Fargate (fully managed)
- Beanstalk vs Lambda
    - Beanstalk can include anything incl. stateful RDS deployments
- Beanstalk vs CloudFormation
    - Beanstalk is for developers; like Heroku, AppEngine, etc.
    - CloudFormation is for ops; doesn’t automate anything; simply define templates in JSON
    - [Source](https://stackoverflow.com/questions/14422151/what-is-the-difference-between-elastic-beanstalk-and-cloudformation-for-a-net-p), [source](https://www.youtube.com/watch?v=SpKqpZW0JUU)
- Beanstalk vs ECS
    - Beanstalk built on ECS ([source](https://fortyft.com/posts/elastic-beanstalk-vs-ecs-vs-kubernetes/))
- Lambda, Fargate are both serverless compute
    - Great comparison from [https://www.youtube.com/watch?v=G4sD-bp2RJk&list=WL&index=19](https://www.youtube.com/watch?v=G4sD-bp2RJk&list=WL&index=19)
        
        ![image (1).png](Devops%20231feabf97a54f3b846175ab2f420999/image_(1).png)
        
    - Great comparison from [https://www.youtube.com/watch?v=DREAmAXFJC8&list=WL&index=20](https://www.youtube.com/watch?v=DREAmAXFJC8&list=WL&index=20) (but seems you can [now deploy custom containers](https://aws.amazon.com/blogs/aws/new-for-aws-lambda-container-image-support/))
        
        ![image (2).png](Devops%20231feabf97a54f3b846175ab2f420999/image_(2).png)
        
    - Integration with AWS means, e.g., “run this Lambda on S3 upload”

## Resources

- Great overview of EKS, ECS, Fargate, ECR: [https://www.youtube.com/watch?v=AYAh6YDXuho&list=WL&index=4](https://www.youtube.com/watch?v=AYAh6YDXuho&list=WL&index=4)

# Kubernetes

## Tips

- `kubectl top pods` and `kubectl top nodes`
- `kubectl describe nodes`  for full listing
- `kubectl get po -o custom-columns="Name:metadata.name,CPU-limit:spec.containers[*].resources.requests.cpu" --all-namespaces` (or `.limits.`)
- You can’t `exec` into a pod as root if it wasn’t started with the root user. You have to use [https://stackoverflow.com/questions/42793382/exec-commands-on-kubernetes-pods-with-root-access](https://stackoverflow.com/questions/42793382/exec-commands-on-kubernetes-pods-with-root-access)
- Deploy a web app replica set behind a load balancer: [docs](https://kubernetes.io/docs/tasks/access-application-cluster/service-access-application-cluster/), [docs](https://kubernetes.io/docs/tutorials/stateless-application/expose-external-ip-address/), [docs](https://aws.amazon.com/premiumsupport/knowledge-center/eks-kubernetes-services-cluster/)
- `apply` is declarative, `create` is imperative - just two different approaches
    - Updating things atomically seems to only really be possible with `apply`. If you created something on CLI without a manifest, like `kubectl create secret` , then you can use a `--dry-run -o yaml | apply` trick: [SO](https://stackoverflow.com/questions/45879498/how-can-i-update-a-secret-on-kubernetes-when-it-is-generated-from-a-file)
- `kubectl rollout restart deployment my-deployment`
- Run a temp shell pod: `kubectl run my-shell --rm -i --tty --image ubuntu -- bash` ([source](https://gc-taylor.com/blog/2016/10/31/fire-up-an-interactive-bash-pod-within-a-kubernetes-cluster))
- Run a shell on a running pod: `kubectl exec --stdin --tty POD -- /bin/bash`
- Fetch latest image with `:latest` and `imagePullPolicy: Always`
- Use `subPath` to mount a single file: [SO](https://stackoverflow.com/questions/33415913/whats-the-best-way-to-share-mount-one-file-into-a-pod)
- Use `defaultMode: 384` on secrets to chmod 600: [SO](https://stackoverflow.com/questions/61728030/kubernetes-volume-mount-permissions-incorrect-for-secret)
- Can get the current yaml description of a resource with `kubectl get deployment foo -o yaml`. Handy for editing (say) the coredns kube-system service.
- Can get, edit, apply with `kubectl edit`.
- Use `kubectl port-forward pods/PODNAME LOCALPORT:REMOTEPORT` to tunnel into a pod

**Port-forward tunnel to any remote hostname**
From [SO](https://stackoverflow.com/questions/61261403/kubectl-port-forward-to-another-endpoint)

```
kubectl run --env REMOTE_HOST=your.service.com --env REMOTE_PORT=8080 --env LOCAL_PORT=8080 --port 8080 --image marcnuri/port-forward test-port-forward

kubectl port-forward test-port-forward 8080:8080

```

**Get node taints**
create this file and then run: ([SO](https://stackoverflow.com/a/44417478/43118))

```
{{printf "%-50s %-12s\n" "Node" "Taint"}}
{{- range .items}}
    {{- if $taint := (index .spec "taints") }}
        {{- .metadata.name }}{{ "\t" }}
        {{- range $taint }}
            {{- .key }}={{ .value }}:{{ .effect }}{{ "\t" }}
        {{- end }}
        {{- "\n" }}
    {{- end}}
{{- end}}

# Then: kubectl get nodes -o go-template-file="./nodes-taints.tmpl"

```

- To use BItnami Sealed Secrets:
    
    ```jsx
    kubeseal -o yaml < raw-secrets.yaml > sealed-secrets.yaml
    ```
    

## Concepts

- Jobs: have `parallelism`

## Helm tips

**Upgrade a package**

```
helm upgrade -f values.yaml --timeout 30m --namespace posthog posthog posthog/posthog --atomic --wait --wait-for-jobs --debug --version 18.0.0

```

## Networking

- `kubectl proxy`: allow connecting to k8s API server at port 8001. For tools like kube-ops-view.

## Best practices

- To ensure daemonsets can run on each node, give them a higher [priority class](https://kubernetes.io/docs/concepts/scheduling-eviction/pod-priority-preemption/)

## Security

- Container escapes are considered vulns
- Sharing hostPath, hostNet, and privileged access are all high-risk
- [Best practices in the docs](https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/#:~:text=Kubernetes%20supports%20encryption%20at%20rest,the%20content%20of%20those%20secrets.)

## Good To Know

- Cannot create configmaps that reflect dir structure ([source](https://stackoverflow.com/questions/55790144/create-configmaps-from-files-recursively))

## Edge cases

- When there are no nodes a pod can be scheduled on, it is in the Pending state
- When master fails, workers continue running. It’s possible to restore? (SO)

## To Learn

- NodePort vs ClusterIP
- Not clear how to debug distroless containers (inspecting files, etc.)

## Core Tools

- Official
    - kubeadm: production
    - kind: run cluster on localhost
    - minikube: run 1-node cluster on localhost
    - kubectl: control client
    - kops: deploy/manage kubeadm on a cluster; “kubectl for clusters”
- k3s: light Kubernetes built by Rancher
- k3d: lightweight wrapper to run k3s
- Helm: templatized Kubernetes YAML files ([video](https://www.youtube.com/watch?v=fy8SHvNZGeE&list=WL&index=29))
    - A way to share “bundles” that others can “install”
    - Helm charts
    - Tiller applies these, but may be removed in Helm 3?
- stern: nice log viewer/tailer that can follow multiple pods: `stern img-optimizer-deployment`
- Packages
    - [https://cert-manager.io/docs/installation/helm/](https://cert-manager.io/docs/installation/helm/)

## Ops tools

- kube-ops-view: for inspecting resource usage/limits across pods
- Kubernetes Dashboard: official web UI
- Kubernetes Lens: have heard about this

## Monitoring tools

- Inspektor Gadget

## Controllers and operators

- Controllers our applications that run in kubernetes that have access to the API and manage resources
    - controllers always are an event Loop that listens to a queue of events from kubernetes.
    - They often use the shared informer, which is a shared cache. This is more scalable than directly interfacing with the kubernetes API as a normal client
    - 
- Controllers are written in any language
- kubebuilder and meta controller are the original frameworks for writing operators
- Operators are a type of controller that manage stateful apps
- You can use newer frameworks like kudo or operator framework
- Publish to operator hub, which you automatically get approved for if you use operator framework
- Alternatives to writing operators include doing nothing, using helm charts, using controllers and crds, modifying the stateful application itself, and services that run outside kubernetes. [Source](https://youtu.be/CwVCfl4qpdg)

## CD

- Argo
- Flux (merged into Argo?)
- Spinnaker: handles complex deployments, like multi-stage canary deployments that wait for time before rolling out more broadly

## Development, preview, and (generally) ephemeral environments

- Skaffold: simply Auto builds and deploys images based on watching file changes, so that you can have your changes automatically running in a kubernetes environment as you are developing. A bit slow since it goes through the full build and deploy process, unlike Devspace.
- DevSpace: similar to Skaffold but also supports file syncing into the remote container for faster changes?
- Tilt: dev environments, open-source, has UI, supports fast file syncing as well, plus a bunch of other stuff
- Proprietary
    - Okteto: dev & preview environments
    - ReleaseHub: preview environments
    - Shipyard: ephemeral environments
    - Bunnyshell: ???

## Landscape

[Landscape](https://karlkfi.medium.com/compendium-of-kubernetes-application-deployment-tools-80a828c91e8f)
**Container Image Building**

- [**Moby**](https://github.com/moby/moby) **/** [**buildkit**](https://github.com/moby/buildkit) (Docker) — A toolkit for converting source code to build artifacts.
- [**kaniko**](https://github.com/GoogleContainerTools/kaniko) (Google) — A tool to build container images from a Dockerfile, inside a container or Kubernetes cluster.
- [**img**](https://github.com/genuinetools/img) (Jess Frazelle) — A standalone, daemon-less, unprivileged Dockerfile and OCI compatible container image builder.
- [**buildah**](https://github.com/containers/buildah) (IBM/Red Hat) — A tool that facilitates building Open Container Initiative (OCI) container images.
- [**Source-To-Image**](https://github.com/openshift/source-to-image) (IBM/Red Hat) — A tool for building artifacts from source and injecting into container images.
- [**Tanzu Build Service**](https://tanzu.vmware.com/build-service) **/** [**kpack**](https://github.com/pivotal/kpack) **/** [**pack**](https://github.com/buildpacks/pack) (VMware/Pivotal) — A CLI and service for building apps using Cloud Native Buildpacks.
- [**Carvel**](https://carvel.dev/) **/** [**kbld**](https://github.com/k14s/kbld) (VMware/Pivotal) — A service for building and pushing images into development and deployment workflows.
- [**Google Cloud Buildpacks**](https://github.com/GoogleCloudPlatform/buildpacks) (Google) — Builders and buildpacks designed to run on Google Cloud’s container platforms.
- [**Makisu**](https://github.com/uber/makisu) (Uber) — A fast and flexible Docker image building tool, that works in unprivileged containerized environments like Mesos and Kubernetes.

**Resource Templating**

- [**Helm**](https://github.com/helm/helm) (Microsoft, Google) — A Kubernetes Package Manager
- [**Kustomize**](https://github.com/kubernetes-sigs/kustomize) (Google, Apple) — A CLI to customize raw, template-free YAML files, leaving the original YAML untouched and usable as-is.
- [**Carvel**](https://carvel.dev/) **/** [**ytt**](https://github.com/k14s/ytt) (VMware/Pivotal) — A YAML templating tool that works on YAML structure instead of text
- [**jsonnet**](https://github.com/google/jsonnet) **/** [**go-jsonnet**](https://github.com/google/go-jsonnet) (Google) — A JSON templating language.
- [**gomplate**](https://github.com/hairyhenderson/gomplate) (Dave Henderson) — A CLI for golang template rendering, supporting local and remote datasources.
- [**Mustache**](https://mustache.github.io/) (Github) — A framework-agnostic JSON templating engine.

**Package Management**

- [**Helm**](https://github.com/helm/helm) (Microsoft, Google) — A Kubernetes Package Manager
- [**Cloud Native Application Bundles (CNAB)**](https://github.com/cnabio/cnab-spec) **/** [**Porter**](https://github.com/deislabs/porter) **/** [**Duffle**](https://github.com/cnabio/duffle) (Microsoft/Deis, Docker) — A package format specification, bundler, and installer for managing cloud agnostic distributed applications.

**Continuous Deployment**

- [**Spinnaker**](https://github.com/spinnaker/spinnaker) (Netflix, Google) — A multi-cloud continuous delivery platform for releasing software changes with high velocity and confidence.
- [**Terraform Kubernetes Provider**](https://github.com/hashicorp/terraform-provider-kubernetes) (Hashicorp) — A [Terraform](https://github.com/hashicorp/terraform) plugin that enables full lifecycle management of Kubernetes resources.
- [**Concourse**](https://github.com/concourse/concourse) (VMware/Pivotal) — A container-based continuous thing-doer written in Go and Elm.
- [**JenkinsX**](https://github.com/jenkins-x/jx) (CloudBees) — An automated CI/CD for Kubernetes with preview environments on pull requests using [Tekton](https://github.com/tektoncd), [Knative](https://github.com/knative/serving), [Lighthouse](https://github.com/submariner-io/lighthouse), [Skaffold](https://github.com/GoogleContainerTools/skaffold) and Helm
- [**Argo CD**](https://github.com/argoproj/argo-cd/) (Intuit) — Adeclarative, GitOps continuous delivery tool for Kubernetes.
- [**Tekton**](https://github.com/tektoncd) / [Tekton Pipelines](https://github.com/tektoncd/pipeline) (Google) — A Kubernetes controller providing CI/CD-style pipeline resources.
- [**Cloud Build**](https://cloud.google.com/cloud-build/) (Google) — A service that executes builds on Google Cloud Platform infrastructure.
- [**Skaffold**](https://github.com/GoogleContainerTools/skaffold) (Google) — A CLI that facilitates continuous development for Kubernetes applications.
- [**Azure DevOps**](https://docs.microsoft.com/en-us/azure/devops/) **/** [**Azure Pipelines**](https://docs.microsoft.com/en-us/azure/devops/pipelines/) (Microsoft) — A cloud service that automatically builds and tests your project code and makes it available to other users.
- [**Brigade**](https://github.com/brigadecore/brigade/) (Microsoft) — Event-based Scripting for Kubernetes.
- [**Habitat**](https://github.com/habitat-sh/habitat) **/** [**habitat-operator**](https://karlkfi.medium.com/github.com/habitat-sh/habitat-operator) (Chef) — A Kubernetes controller that runs and manages Habitat Services on Kubernetes.
- [gitkube](https://github.com/hasura/gitkube) (Hasura) — A tool for building and deploying Docker images on Kubernetes using git push.

**Imperative Deployment**

- [**Kubebuilder**](https://github.com/kubernetes-sigs/kubebuilder) (CNCF, Google, Apple, IBM/Red Hat) — An SDK for building Kubernetes APIs (and controllers and operators) using CRDs.
- [**Operator Framework**](https://operatorframework.io/) **/** [**Operator SDK**](https://github.com/operator-framework/operator-sdk) (IBM/Red Hat/CoreOS) — An SDK for building Kubernetes application operators.
- [**KUDO**](https://github.com/kudobuilder/kudo) (D2IQ) — A framework for building production-grade Kubernetes Operators using a declarative approach.
- [**Pulumi**](https://github.com/pulumi/pulumi) ([Pulumi](https://www.pulumi.com/)) — An Infrastructure as Code SDK for creating and deploying cloud software that use containers, serverless functions, hosted services, and infrastructure, on any cloud.
- [**Carvel**](https://carvel.dev/) **/** [**kapp**](https://get-kapp.io/) **/** [**kapp-controller**](https://github.com/k14s/kapp-controller) (VMware/Pivotal) — A CLI and Kubernetes controller for installing configuration (helm charts, ytt templates, plain yaml) as described by App CRD.
- [**Isopod**](https://github.com/cruise-automation/isopod) (Cruise) — An expressive DSL and framework for Kubernetes resource configuration without YAML.

**Autoscaling**

- [**Horizontal Pod Autoscaler**](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/) (built-in) — A Kubernetes controller that automatically scales the number of pods in a replication controller, deployment, replica set or stateful set based on a configured metric.
- [**Vertical Pod Autoscaler**](https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler) (Google) — A set of Kubernetes components that automatically adjusts the amount of CPU and memory requested by pods running in the Kubernetes Cluster.
- [**Addon Resizer**](https://github.com/kubernetes/autoscaler/tree/master/addon-resizer) (Google) — A simplified version of vertical pod autoscaler that modifies resource requests of a deployment based on the number of nodes in the Kubernetes Cluster.
- [**KEDA**](https://github.com/kedacore/keda) (Microsoft) — A Kubernetes-based Event Driven Autoscaling component.
- [**Watermark Pod Autoscaler Controller**](https://github.com/DataDog/watermarkpodautoscaler) (DataDog) — A custom controller that extends the Horizontal Pod Autoscaler (HPA).
- [**Pangolin**](https://github.com/dpeckett/pangolin) (Damian Peckett) — An enhanced Horizontal Pod Autoscaler for Kubernetes that scales deployments based on their Prometheus metrics, using a variety of highly configurable control strategies.
- [**Predictive Horizontal Pod Autoscaler**](https://github.com/jthomperoo/predictive-horizontal-pod-autoscaler) (IBM) — A custom pod autoscaler, similar to Horizontal Pod Autoscaler, however with added predictive elements.
- [**Horizontal Pod Autoscaler Operator**](https://github.com/banzaicloud/hpa-operator) (Banzai Cloud) — A Kubernetes controller that watches Deployments or StatefulSets and automatically creates HorizontalPodAutoscaler resources, based on autoscale annotations.

## Components

- Pod: smallest unit
    - Usually 1 app per pod: 1 main container + sidecars
    - Has own IP
- Service: pod load balancer with permanent IP
    - A stable “location” to put in front of pods, which are ephemeral
    - External and internal services
- Cluster: of services
- Ingress: forwards network requests to an (external) service
- ConfigMap: simple metadata store (e.g. app pods needing to connect to DB can request `DB_URL` to find the service name)
    - Not for secrets
- Secret: like ConfigMap but encrypted and b64
- Volumes: for persistence
- Deployment: a blueprint for replicating stateless pods; you’d directly create deployments, not pods
- Stateful sets: a blueprint for replicating stateful pods like DBs; ensures writes are synchronized to avoid concurrent DB writes
    - These are challenging to use; commonly host DB outside K8S
- Namespace: organize your clusters
- Operator: manages stateful applications ([source](https://www.youtube.com/watch?v=ha3LjlD6g7g&list=WL&index=24))

## More concepts

- Taints and tolerances let you *allow only certain pods* to run on a node

## Resources

- [Overview](https://www.youtube.com/watch?v=X48VuDVv0do&t=1s)
- Set up cluster on bare EC2 in 5min: [video](https://www.youtube.com/watch?v=vpEDUmt_WKA&list=WL&index=53)

## Services comparison

From [HN](https://news.ycombinator.com/item?id=26299567):

> If you use GKE, you have been spoiled. Other Kubernetes platforms - in particular EKS, which is what most people are going to use since most people use AWS - are way more work to setup and maintain. (Emphasis on the "maintain".)
> 

From [Reddit](https://www.reddit.com/r/googlecloud/comments/ejxxn5/has_anyone_done_a_thorough_diff_analysis_between/fd47cda/?utm_source=share&utm_medium=web2x):

Speak to anyone who's been around kubernetes and they will tell you that GKE is much easier to setup and operate than EKS. This is in part because GCP has some better cross zone and cross region primitives than AWS, and also because AWS seems in no particular hurry to reduce the operational burden for EKS but it is improving.

The difficulty is increased due to the lack of integration among AWS primitives arising from their two pizza team strategy vs GCP's more refined approach (there are many areas where AWS is better than GCP though (such as Route 53, S3, RDS, lambda)). The lack of integration is exhibited in the complex terraform needed to support EKS whereas you don't need a terraform module to support GKE - you can easily use the GCP provider primitives.

[terraform-aws-modules/terraform-aws-eks#635](https://github.com/terraform-aws-modules/terraform-aws-eks/issues/635)

*My feeling is that complexity getting too high and quality is suffering somewhat. We are squeezing a lot of features in a single module. This is by far the most complex module in the Terraform AWS modules org. But perhaps I just feel the pressure since I'm a maintainer?*

Regarding managed kubernetes though, here are some differences off the top of my head:

- On GKE, you don't need to define node-pools if you tick enable node-autoprovisioning. This is effectively an EKS fargate profile without any of the limitations (discrete node size, inability to mount volumes etc)
- On GKE there's no need for bastion hosts, you can connect to private nodes automatically by tunneling through identity-aware-proxy
- On GKE, there's no faffing about with aws-auth-configmap, nodes automatically join the cluster
- On GKE, you can use regional persistent disks if you need to store state rather than single zone EBS
- On GKE, you can use a single static ip for your nginx-ingress load balancer without needing to faff about with AWS global accelerator (which in any case gives you multiple ips)
- On GKE, managed node pools are automatically repaired and upgraded with the choice of google's OS or ubuntu unlike Amazon Linux which is currently the only choice for EKS' managed node pools. GKE can automatically use spot instances unlike the farce on AWS with a 3rd party (spotinst) charging a premium for the same functionality
- On GKE, managed istio is ticking a box versus a self-install on EKS
- On GKE you need to worry much less about ip exhaustion as they use alias ips vs dedicated ENIs for EKS
- Associating IAMs with kuberenetes service accounts is much easier with GKE workload identity than with EKS' oidc webhook
- GKE has several features which EKS doesn't (calico, vertical pod autoscaling, binary authorisation, export of cluster data to bigquery)

# Kubernetes service mesh

- Microservices have various common auxiliary networking problems to solve: communications configuration (locations), secure transport, retries, metrics/tracing
- Sidecar proxy pattern: can handle all this networking logic; run in same pod
- Istio
- Envoy

# Knative

- Components
    - Tekton (spun out)
    - Serving
    - Eventing
- Kills pods if HTTP request disconnects; tightly bound to request ([source](https://stackoverflow.com/questions/65365944/how-to-tells-knative-pod-autoscaler-not-to-kill-in-progress-long-running-pod/65881346#65881346))

# Terraform

- Infrastructure as code
- Platform providers include AWS (IaaS), Kubernetes (PaaS), Fastly (SaaS) etc.
- Declare desired state
- `refresh`, `plan`, and `apply`
- Comparisons
    - vs CloudFormation: cross-platform
    - vs Ansible: infrastructure, not OS

# TODO

- Terraform

# Identity services

[https://news.ycombinator.com/item?id=26336033](https://news.ycombinator.com/item?id=26336033)

[https://news.ycombinator.com/item?id=22871180](https://news.ycombinator.com/item?id=22871180)

[https://news.ycombinator.com/item?id=26069621](https://news.ycombinator.com/item?id=26069621)

[https://news.ycombinator.com/item?id=26409820](https://news.ycombinator.com/item?id=26409820)

[https://github.com/authelia/authelia](https://github.com/authelia/authelia)

[https://news.ycombinator.com/item?id=26409820](https://news.ycombinator.com/item?id=26409820)

# Nginx

Some helpful proxy_pass headers

```
  proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
  proxy_set_header Upgrade $http_upgrade;
  proxy_set_header Connection "upgrade";
  proxy_set_header Host $host;
  proxy_read_timeout 300;
  proxy_connect_timeout 300;
  proxy_send_timeout 300;

```